{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from network.cnn_net import cnn_net\n",
    "from sklearn.decomposition import PCA\n",
    "import utils.mnist_loader as mnist_loader\n",
    "import utils.mnistf_loader as mnistf_loader\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import argparse\n",
    "from numpy import ma\n",
    "import scipy\n",
    "import os.path\n",
    "import network.ann_net as ann_net\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "def gn_adv(args, model, device, target):\n",
    "    \"\"\"\n",
    "\n",
    "    :param args:\n",
    "    :param model:\n",
    "    :param device:\n",
    "    :param target: label for which adv image is to be generated\n",
    "    :return: adv image\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    target = torch.Tensor([target]).long()\n",
    "    data = torch.rand((1, 1, 28, 28), requires_grad=True, device=device)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer = optim.SGD([data], lr=1, momentum=args.momentum)\n",
    "\n",
    "    for itr in range(50):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        #print(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    return torch.squeeze(data).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def sneaky_adv(args, model, device, img, target, lmbd):\n",
    "    \"\"\"\n",
    "    :param img: image whose alike image to be constructed\n",
    "    :param target: label that network should output\n",
    "    :return: image that looks like img and network output is target\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    data = torch.rand((1, 1, 28, 28), device=device, requires_grad=True)\n",
    "    target = torch.Tensor([target]).long()\n",
    "    img = torch.Tensor(img).view((1, 1, 28, 28))\n",
    "    img, target, data = img.to(device), target.to(device), data.to(device)\n",
    "    optimizer = optim.SGD([data], lr=0.4, momentum=args.momentum)\n",
    "\n",
    "    for itr in range(1500):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target) + lmbd*torch.norm((img-data), 2)\n",
    "        loss.backward()\n",
    "        #print(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    return torch.squeeze(data).detach().cpu().numpy()\n",
    "    \n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def kld(model, actual):\n",
    "    \"\"\"\n",
    "    model : its the array that is input to the network\n",
    "    actual : image corresponding to the network output\n",
    "    return: KL divergence between two prob distributions\n",
    "    \"\"\"\n",
    "    model = np.asarray(list(model) + [0 for i in range(255 - len(model))])\n",
    "    actual = np.asarray(list(actual) + [0 for i in range(255 - len(actual))])\n",
    "    k = ((model * np.log(model)) - (model * np.log(actual))).sum()\n",
    "    print(k)\n",
    "    # k = (model * ma.log(model/actual)).sum()\n",
    "\n",
    "    return k\n",
    "\n",
    "\n",
    "def mean_image(tr_data):\n",
    "    \"\"\"\n",
    "\n",
    "    :param tr_data: input mnist data\n",
    "    :return: mean images for each class\n",
    "    \"\"\"\n",
    "    val = {}\n",
    "    for i in range(10):\n",
    "        mat = []\n",
    "        for r in range(len(tr_data)):\n",
    "            if tr_data[r][1][i] == 1:\n",
    "                x = tr_data[r][0]\n",
    "                mat.append(np.asarray(x))\n",
    "        val[i] = np.mean(mat, axis=0) * (100)\n",
    "\n",
    "    return val\n",
    "\n",
    "\n",
    "def gn_idx_list(training_data):\n",
    "    idx_list = {}\n",
    "    for i in range(10):\n",
    "        idx_list[i] = []\n",
    "    for i in range(len(training_data)):\n",
    "        idx_list[np.argmax(training_data[i][1])].append(i)\n",
    "\n",
    "    return idx_list    \n",
    "\n",
    "\n",
    "\n",
    "def gn_adv_imgs(args, model, device, training_data):\n",
    "    \"\"\"\n",
    "\n",
    "    :param training_data:\n",
    "    :return: 100 adversarial images\n",
    "    \"\"\"\n",
    "    gn = {}\n",
    "    \"\"\"idx_list = []\n",
    "    for i in range(10):\n",
    "        idx = np.random.randint(0, 8000)\n",
    "        while training_data[idx][1][i] != 1:\n",
    "            idx += 1\n",
    "        idx_list.append(idx)\"\"\"\n",
    "    idx_list = gn_idx_list(training_data)\n",
    "\n",
    "    for g in range(10):\n",
    "        gl = []\n",
    "        print(g)\n",
    "        for v in range(10):\n",
    "            if g != v:\n",
    "                for i in range(400):\n",
    "                    idx = idx_list[v][i]\n",
    "                    gl.append(sneaky_adv(args, model, device, training_data[idx][0], g, 0.4))\n",
    "            else:\n",
    "                for i in range(400):\n",
    "                    idx = idx_list[v][i]\n",
    "                    gl.append(gn_adv(args, model, device, g))\n",
    "        gn[g] = gl\n",
    "\n",
    "    return gn\n",
    "\n",
    "\n",
    "def gn_pd_imgs(adv_imgs):\n",
    "    \"\"\"\n",
    "    adv_imgs: a dictionary with keys as the classes and corresponding adv images as values\n",
    "    returns gn_pd : a dictionary with keys as classes and corresponding prob. distribution of adv images\n",
    "    \"\"\"\n",
    "    gn_pd = {}\n",
    "    for k, v in adv_imgs.items():\n",
    "        bin_y = []\n",
    "\n",
    "        for i in range(len(v)):\n",
    "            bin_y.append(generate_pd(v[i]))\n",
    "\n",
    "        gn_pd[k] = bin_y\n",
    "\n",
    "    return gn_pd\n",
    "\n",
    "\n",
    "def mean_pd(mean_imgs):\n",
    "    \"\"\"\n",
    "    val: a dictionary with keys as the classes and corresponding mean images as values\n",
    "    returns val_pd : a dictionary with keys as classes and corresponding prob. distribution of mean images\n",
    "    \"\"\"\n",
    "    val_pd = {}\n",
    "    for k, v in mean_imgs.items():\n",
    "        # print(k)\n",
    "        # print(len(v))\n",
    "        val_pd[k] = generate_pd(v)\n",
    "\n",
    "    return val_pd\n",
    "\n",
    "\n",
    "def generate_pd(img, patch_size=28):\n",
    "    \"\"\"\n",
    "    img: an image\n",
    "    returns pd : prob distribution of intensities in img\n",
    "    \"\"\"\n",
    "    \n",
    "    if img.max() - img.min() == 0:\n",
    "        img = np.zeros((patch_size, patch_size))\n",
    "    else:\n",
    "        img = ((img - img.min()) * (1 / (img.max() - img.min()) * 255).astype('uint8'))\n",
    "    img = np.floor(img)\n",
    "    img = img.reshape((-1, patch_size**2))\n",
    "    img = img.astype('int64')\n",
    "    bin_m = np.bincount(img[0])\n",
    "    l1 = list(bin_m)\n",
    "    l2 = [0]*(256 - len(bin_m))\n",
    "    bin_m = l1 + l2\n",
    "    bin_m = [x if x != 0 else 0.0001 for x in bin_m] \n",
    "    bin_m = bin_m / sum(bin_m)\n",
    "    # making sure to have len of list 255\n",
    "    pd = np.asarray(bin_m)\n",
    "    #pd = [x if x != 0 else 0.0001 for x in pd]  # to avoid divide by zero in KLD\n",
    "\n",
    "    return pd\n",
    "\n",
    "\n",
    "\n",
    "def kl_calc(adv_im, mean_im):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    adv_im = np.asarray(adv_im).reshape(28, 28)\n",
    "    mean_im = np.asarray(mean_im).reshape(28, 28)\n",
    "    k_sum = 0\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            s_arr = adv_im[i: i + 7, i: i + 7]\n",
    "            p = generate_pd(s_arr, 7)\n",
    "            # mean image pd\n",
    "            ms_arr = mean_im[i: i + 7, i: i + 7]\n",
    "            q = generate_pd(ms_arr, 7)\n",
    "            k_sum += scipy.stats.entropy(p, q)\n",
    "\n",
    "    return k_sum\n",
    "\n",
    "def js_calc(adv_im, mean_im):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    adv_im = np.asarray(adv_im).reshape(28, 28)\n",
    "    mean_im = np.asarray(mean_im).reshape(28, 28)\n",
    "    k_sum = 0\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            s_arr = adv_im[i: i + 7, j: j + 7]\n",
    "            p = generate_pd(s_arr, 7)\n",
    "            # mean image pd\n",
    "            ms_arr = mean_im[i: i + 7, j: j + 7]\n",
    "            q = generate_pd(ms_arr, 7)\n",
    "            r = (p + q)/ 2\n",
    "            k_sum += (scipy.stats.entropy(p, r) / 2) + (scipy.stats.entropy(q, r) / 2)\n",
    "\n",
    "    return k_sum\n",
    "\n",
    "def ac_calc(test_data, training_data, adv_imgs, mean_imgs, min_thr, fun):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    min_thr = min_thr_calc(adv_imgs, mean_imgs, fun)\n",
    "    \n",
    "    success = 0\n",
    "    tmp = 1000\n",
    "    for i in range(len(adv_imgs)):\n",
    "        f = adv_imgs[i]\n",
    "        for j in range(len(f)):\n",
    "            tmp = fun(f[j],mean_imgs[i])\n",
    "            if tmp >= min_thr[i]:\n",
    "                success += 1\n",
    "\n",
    "    #print((success/40000) * 100)\n",
    "    #success = 0\n",
    "    tmp = 1000\n",
    "    for i in range(400):\n",
    "        label = np.argmax(test_data[i][1])\n",
    "        f = np.asarray(test_data[i][0]).reshape((-1, 784))\n",
    "        tmp = fun(f, mean_imgs[label])\n",
    "        if tmp < min_thr[label]:\n",
    "            success += 1\n",
    "    '''\n",
    "    for i in range(len(training_data)):\n",
    "        label = np.argmax(training_data[i][1])\n",
    "        f = np.asarray(training_data[i][0]).reshape((-1, 784))\n",
    "        tmp = fun(f, mean_imgs[label])\n",
    "        if tmp < min_thr[label]:\n",
    "            success += 1\n",
    "'''\n",
    "    return (success/(len(adv_imgs)*len(adv_imgs[0]) + 400)* 100)\n",
    "\n",
    "\n",
    "def min_thr_calc(adv_imgs, mean_imgs, fun):\n",
    "    \"\"\"\n",
    "    # each class contain 4000 adversarial examples\n",
    "    # 400 uniformly chosen samples out of 4000 are used for min_thr calc\n",
    "    return: list containing min_thr for each of the 10 classes\n",
    "    \"\"\"\n",
    "    min_thr = []\n",
    "    ind = np.random.randint(40, size=4)\n",
    "    tmp = 1000\n",
    "    for i in range(len(adv_imgs)):\n",
    "        f = adv_imgs[i]\n",
    "        for j in ind:\n",
    "            tmp = min(tmp, fun(f[j], mean_imgs[i]))\n",
    "        if (tmp == 1000):\n",
    "            print(i)\n",
    "        min_thr.append(tmp) \n",
    "        tmp =1000\n",
    "    \n",
    "    return min_thr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1]])\n",
      "file_does_not_exist\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#def main():\n",
    "    # Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=50, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=True,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "\n",
    "parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                    help='For Saving the current Model')\n",
    "args = parser.parse_args([])\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('data/mnist_fashion', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('data/mnist_fashion', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "model = cnn_net().to(device)\n",
    "optimizer = None\n",
    "\n",
    "# for training of the cnn-network\n",
    "if (args.save_model):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader)\n",
    "\n",
    "    torch.save(model.state_dict(), \"mnistf_cnn.pt\")\n",
    "\n",
    "# to generate adversarial images from trained network\n",
    "model.load_state_dict(torch.load(\"mnistf_cnn.pt\"))\n",
    "model.eval()\n",
    "\n",
    "#training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "training_data = mnistf_loader.load_mnist('data/mnist_fashion', kind='train')\n",
    "test_data = mnistf_loader.load_mnist('data/mnist_fashion', kind='t10k')\n",
    "training_data = list(training_data)\n",
    "test_data = list(test_data)\n",
    "\n",
    "'''for target in range(10):\n",
    "    res = gn_adv(args, model, device, target)\n",
    "    print(res.shape)\n",
    "    res = torch.Tensor(res).reshape(2, 1, 28, 28)\n",
    "    output = model(res)\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    print(pred)\n",
    "'''\n",
    "# generate sneaky_adversarial images test code\n",
    "target_label = 1\n",
    "target_img = 4\n",
    "idx = np.random.randint(0, 8000)\n",
    "while training_data[idx][1][target_img] != 1:\n",
    "    idx += 1\n",
    "#training_data[idx][0]\n",
    "res = sneaky_adv(args, model, device, training_data[idx][0], target_label, 0.3)\n",
    "plt.imshow(res.reshape(28, 28), cmap=\"Greys\")\n",
    "res = torch.Tensor(res).reshape(1, 1, 28, 28)\n",
    "output = model(res)\n",
    "pred = output.argmax(dim=1, keepdim=True)\n",
    "print(pred)\n",
    "\n",
    "# generate adversarial images\n",
    "if os.path.exists('adv_imgs_mnistf.pkl'):\n",
    "    print(\"file_exist\")\n",
    "    with open('adv_imgs_imgs_mnistf.pkl', 'rb') as f:\n",
    "        adv_imgs = pickle.load(f)\n",
    "else:\n",
    "    print(\"file_does_not_exist\")\n",
    "    adv_imgs = gn_adv_imgs(args, model, device, training_data)\n",
    "    with open(\"adv_imgs_mnistf.pkl\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(adv_imgs, fp)\n",
    "print(len(adv_imgs[0]))\n",
    "#x = adv_imgs[1]\n",
    "#y = x[800]\n",
    "#plt.imshow(y.reshape(28, 28), cmap='Greys')\n",
    "#prb = generate_pd(training_data[1][0]*100)\n",
    "\n",
    "\n",
    "'''success = 0\n",
    "for i in range(10):\n",
    "    for j in range(len(adv_imgs[i])):\n",
    "        res = adv_imgs[i][j]\n",
    "        res = torch.Tensor(res).reshape(1, 1, 28, 28)\n",
    "        output = model(res)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        if pred == i:\n",
    "            success += 1\n",
    "\n",
    "print((success/ 40000) * 100)\n",
    "#print(pred)\n",
    "res = res.detach().numpy()\n",
    "#plt.imshow(res.reshape(28, 28), cmap=\"Greys\")\n",
    "#plt.show()\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''mean_imgs = mean_image(training_data)\n",
    "#y = mean_imgs[2]\n",
    "#plt.imshow(y.reshape(28, 28), cmap='Greys')\n",
    "idx_l = gn_idx_list(training_data)\n",
    "'''\n",
    "\n",
    "'''fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(tr_l, np.zeros((len(tr_l))), s=10, c='b', marker=\"s\", label='train')\n",
    "ax1.scatter(ad_l, np.ones((len(ad_l))), s=10, c='r', marker=\"o\", label='adversarial')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()\n",
    "'''    \n",
    "\n",
    "#Calculating threshold\n",
    "#min_thr = min_thr_calc(adv_imgs, mean_imgs, kl_calc) \n",
    "#print(ac_calc(test_data, training_data, adv_imgs, mean_imgs, min_thr, kl_calc))\n",
    "#print(ac_calc(test_data, training_data, adv_imgs, mean_imgs, js_calc))\n",
    "\n",
    "#min_thr = min_thr_calc(adv_imgs, mean_imgs, js_calc) \n",
    "#print(ac_calc(test_data, training_data, adv_imgs, mean_imgs, min_thr, js_calc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fef376f96d8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(training_data[idx][0].reshape(28, 28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_l = []\n",
    "ad_l = []\n",
    "for j in range(10):\n",
    "    lst = idx_l[j]\n",
    "    for i in range(4000):\n",
    "        ind = lst[i]\n",
    "        tr_l.append(js_calc(training_data[ind][0], mean_imgs[j]))\n",
    "    for i in range(4000):\n",
    "        ad_l.append(js_calc(adv_imgs[j][i], mean_imgs[j]))\n",
    "        \n",
    "y = [0]*len(ad_l) + [1]*len(tr_l)\n",
    "x = ad_l + tr_l\n",
    "x2 = [i**2 for i in x]\n",
    "x3 = [i**3 for i in x]\n",
    "data = {'x': x, 'x2': x2, 'x3': x3, 'y': y}\n",
    "df = pd.DataFrame(data)\n",
    "df = df.sample(frac=1).reset_index(drop=True)                   \n",
    "n_df = df[['x', 'x2', 'x3']]\n",
    "normalized_df = (n_df - n_df.mean()) / n_df.std()\n",
    "normalized_df['y'] = df['y']\n",
    "\n",
    "print(df.head(1000))\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg.fit(normalized_df[['x', 'x2', 'x3']][:60000], normalized_df['y'][:60000])\n",
    "\n",
    "res = logreg.predict(normalized_df[['x', 'x2', 'x3']][60000:])\n",
    "Y = np.asarray(normalized_df['y'][60000:])\n",
    "acc = (np.sum(Y == res) / 20000)*100\n",
    "                   \n",
    "                   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
